{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ys7WYqDr1xdA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
        "\n",
        "# The GPU id to use, usually either \"0\" or \"1\";\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H6_RkQGL3EZg",
        "outputId": "78c42ba3-84ca-47ea-f205-7611799ba8e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mPBwaq3M1xdC"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import sklearn\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import string\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from pickle import dump\n",
        "from string import punctuation\n",
        "\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, MaxPooling1D, concatenate\n",
        "\n",
        "\n",
        "from pickle import load\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "import csv, string"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCP-AAf22sUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0kA2HXNx2s1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wt4CIeEq2tk6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kr_ae8oN2uEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi7GY1oT1xdC",
        "outputId": "1cc9ee31-a021-49ce-cbc5-92750cb5337b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Gender abusive hate': 0, 'Geopoitical Hate': 1, 'Religious Hate': 2, 'Political Normal': 3, 'Political Hate': 4, 'Personal Hate': 5}\n"
          ]
        }
      ],
      "source": [
        "categories = ['Gender abusive hate', 'Geopoitical Hate','Religious Hate','Political Normal','Political Hate','Personal Hate']\n",
        "num_of_labels=len(categories)\n",
        "categories_dict={key:value for value,key in enumerate(categories)}\n",
        "categories_inverse_dict={key:value for key,value in enumerate(categories)}\n",
        "print(categories_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "2XqF1xUg4rPw",
        "outputId": "d684c205-8a0a-4874-f61e-1bb750c84d4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg-p4vAM1xdD",
        "outputId": "7505958f-eddb-4327-c2cf-6a019229cb65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['অতএব', 'অথচ']\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "stop_words='/content/drive/MyDrive/Thesis/stopwords-bn.txt'\n",
        "text_data=[]\n",
        "with open(stop_words,'r',encoding='utf-8') as temp_output_file:\n",
        "    reader=csv.reader(temp_output_file, delimiter='\\n')\n",
        "    for row in reader:\n",
        "        text_data.append(row)\n",
        "stop_word_list=[x[0] for x in text_data]\n",
        "print(stop_word_list[0:2])\n",
        "print(type(stop_word_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "D1hLMCnk4Aiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s5a3IRAm4Mix",
        "outputId": "49639c83-feaa-483f-d927-e5369e576b80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nUbl0Hij1xdD"
      },
      "outputs": [],
      "source": [
        "def save_dataset(dataset, filename):\n",
        "    dump(dataset, open(filename,'wb'))\n",
        "    print('Saved :%s' % filename)\n",
        "\n",
        "def load_doc(filename):\n",
        "    file=open(filename,'r',encoding='utf-8')\n",
        "    text=file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "def clean_doc(doc, stop_word_list):\n",
        "    # split into tokens by white space\n",
        "    sentences=list()\n",
        "\n",
        "    for sentence in doc:\n",
        "        # remove punctuation from each token\n",
        "\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        tokens = [w.translate(table) for w in sentence.split(' ')]  # I belive word in sentence\n",
        "        sentences.append(' '.join(tokens))\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Thesis/bengali_hate_v2.0.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path, encoding=\"utf-8\")  # Load the dataset\n",
        "print(df.head())  # Print first few rows to check structure\n",
        "print(df.columns)  # Show column names\n"
      ],
      "metadata": {
        "id": "VUY_LqvD76em",
        "outputId": "6249aa54-4d54-43f1-be3c-05ceceee0131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text         label  target\n",
            "0  বৌদির দুধ দেকে তো আমার ই চোখ ঠিক ছিলো না - পোল...      Personal       0\n",
            "1  এই সরকার কে যারা নির্লজ্জের মত সাপোর্ট দিয়েছে ...     Political       1\n",
            "2  পিলখানা হত্যাকান্ড বাংলাদেশের প্রতিরক্ষা ব্যবস...  Geopolitical       3\n",
            "3  ভারতের অর্থনীতি নিয়ে আপনাদের ভাবতে হবে না। ভা...  Geopolitical       3\n",
            "4            খানকির পুলা মালায়নদের মেরে সাফা করে ফেল      Personal       0\n",
            "Index(['text', 'label', 'target'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_data_and_labels(filename, stop_word_list):\n",
        "    df = pd.read_csv(filename, encoding=\"utf-8\")  # Load CSV properly\n",
        "\n",
        "    x_doc = df[\"text\"].tolist()  # Extract text\n",
        "    label_names = df[\"label\"].tolist()  # Extract labels\n",
        "\n",
        "    categories_dict = {\n",
        "        \"Personal\": 0,\n",
        "        \"Political\": 1,\n",
        "        \"Religious\":2,\n",
        "        \"Geopolitical\": 3\n",
        "\n",
        "    }  # Define your category mapping\n",
        "\n",
        "    labels = []\n",
        "    for label_name in label_names:\n",
        "        if label_name not in categories_dict:\n",
        "            print(f\"Warning: Label '{label_name}' not found in categories_dict\")\n",
        "            continue  # Skip unknown labels\n",
        "        labels.append(categories_dict[label_name])\n",
        "\n",
        "    trainX = clean_doc(x_doc, stop_word_list)  # Clean text\n",
        "    return trainX, labels\n",
        "\n",
        "X_train, y_train = load_data_and_labels('/content/drive/MyDrive/Thesis/bengali_hate_v2.0.csv', stop_word_list)\n"
      ],
      "metadata": {
        "id": "a9Dbvm3q9f2V"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_data_and_labels(filename, stop_word_list):\n",
        "    df = pd.read_csv(filename, encoding=\"utf-8\")  # Load CSV properly\n",
        "    print(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.\")  # Debugging\n",
        "\n",
        "    x_doc = df[\"text\"].tolist()  # Extract text\n",
        "    label_names = df[\"label\"].tolist()  # Extract labels\n",
        "\n",
        "    categories_dict = {\n",
        "        \"Personal\": 0,\n",
        "        \"Political\": 1,\n",
        "        \"Religious\": 2,\n",
        "        \"Geopolitical\": 3\n",
        "    }  # Define your category mapping\n",
        "\n",
        "    labels = []\n",
        "    unknown_labels = 0\n",
        "\n",
        "    for label_name in label_names:\n",
        "        if label_name not in categories_dict:\n",
        "            unknown_labels += 1\n",
        "            continue  # Skip unknown labels\n",
        "        labels.append(categories_dict[label_name])\n",
        "\n",
        "    print(f\"Processed {len(labels)} labels. Skipped {unknown_labels} unknown labels.\")  # Debugging\n",
        "\n",
        "    trainX = clean_doc(x_doc, stop_word_list)  # Clean text\n",
        "    print(f\"Preprocessed text data: {len(trainX)} samples.\")  # Debugging\n",
        "\n",
        "    return trainX, labels\n",
        "\n",
        "X_train, y_train = load_data_and_labels('/content/drive/MyDrive/Thesis/bengali_hate_v2.0.csv', stop_word_list)\n",
        "\n",
        "print(f\"X_train sample: {X_train[:5]}\")  # Show first 5 processed texts\n",
        "print(f\"y_train sample: {y_train[:5]}\")  # Show first 5 labels\n"
      ],
      "metadata": {
        "id": "vhiIhk2B-o2K",
        "outputId": "f047f8eb-449b-4bc7-96b3-2ca03b889b3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully with 5698 rows and 3 columns.\n",
            "Processed 5698 labels. Skipped 0 unknown labels.\n",
            "Preprocessed text data: 5698 samples.\n",
            "X_train sample: ['বৌদির দুধ দেকে তো আমার ই চোখ ঠিক ছিলো না  পোলাপান এর চোখ কিভাবে ঠিক থাকবে', 'এই সরকার কে যারা নির্লজ্জের মত সাপোর্ট দিয়েছে বছরের পর বছর তাদের আরো এমন রাস্তায় রাস্তায় কাঁদতে হবে ', 'পিলখানা হত্যাকান্ড বাংলাদেশের প্রতিরক্ষা ব্যবস্থা ধ্বংসের জন্য ভারতের প্রত্যক্ষ সহযোগিতায় এই হত্যাকা ঘটানো হয়েছিল ', 'ভারতের অর্থনীতি নিয়ে আপনাদের ভাবতে হবে না। ভারতের অর্থনীতি নিয়ে ভারত সরকার আছে। আদার বেপারী জাহাজের খোঁজ নিয়ে লাভ নাই।', 'খানকির পুলা মালায়নদের মেরে সাফা করে ফেল']\n",
            "y_train sample: [0, 1, 3, 3, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rg8PAFCH1xdD"
      },
      "outputs": [],
      "source": [
        "def load_data_and_labels(filename, stop_word_list):\n",
        "    doc = load_doc(filename)\n",
        "    x_doc = []\n",
        "    label = []\n",
        "\n",
        "    first_line = True  # Flag to skip header\n",
        "\n",
        "    for line in doc.split('\\n'):\n",
        "        if first_line:  # Skip the first line (header)\n",
        "            first_line = False\n",
        "            continue\n",
        "\n",
        "        x_and_label = line.split(',')\n",
        "\n",
        "        # Ensure there are at least two elements (text + label)\n",
        "        if len(x_and_label) < 2:\n",
        "            continue  # Skip malformed lines\n",
        "\n",
        "        text, label_name = x_and_label[1], x_and_label[0]\n",
        "\n",
        "        if label_name not in categories_dict:\n",
        "            #print(f\"Warning: Label '{label_name}' not found in categories_dict\")  # Debugging\n",
        "            continue  # Skip unknown labels\n",
        "\n",
        "        x_doc.append(text)\n",
        "        label.append(categories_dict[label_name])  # Convert label to numeric\n",
        "\n",
        "    trainX = clean_doc(x_doc, stop_word_list)\n",
        "    return trainX, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sttHx-cj1xdD"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = load_data_and_labels('/content/drive/MyDrive/Thesis/bengali_hate_v2.0.csv',stop_word_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCU1Uyim1xdD",
        "outputId": "9dcdaf68-42de-4dd3-a6e8-3fe163f9d779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples before cleaning: 5698\n",
            "First 10 raw texts: ['বৌদির দুধ দেকে তো আমার ই চোখ ঠিক ছিলো না - পোলাপান এর চোখ কিভাবে ঠিক থাকবে!', 'এই সরকার কে যারা নির্লজ্জের মত সাপোর্ট দিয়েছে বছরের পর বছর, তাদের আরো এমন রাস্তায় রাস্তায় কাঁদতে হবে ', 'পিলখানা হত্যাকান্ড বাংলাদেশের প্রতিরক্ষা ব্যবস্থা ধ্বংসের জন্য ভারতের প্রত্যক্ষ সহযোগিতায় এই হত্যাকা- ঘটানো হয়েছিল ', 'ভারতের অর্থনীতি নিয়ে আপনাদের ভাবতে হবে না। ভারতের অর্থনীতি নিয়ে ভারত সরকার আছে। আদার বেপারী জাহাজের খোঁজ নিয়ে লাভ নাই।', 'খানকির পুলা মালায়নদের মেরে সাফা করে ফেল', 'শুয়োর এর বাচ্চা দুরগারে চুদি কুত্তা দিয়া কালিরে চুদি পাঠা দিয়া তোর মারে চুদি আমার সোনা দিয়া সিং', 'মাগীর পাছায় লাথি মেরে ওই প্লাটফর্মে রেখে আসা উচিত ', 'আমরা তো দেখলাম ইউটিউব এর মাধ্যমে গাড়ি আওয়ামী লীগ এর লোকজনে পোড়ালো। অথচ মামলা করে দিলো বিএনপির নামে।', 'সানিউর কুত্তার বাচ্চা, তুর বোনেরে চুদমু তুর কান্দে ফালাইয়া, তুর মাইরে চুদমো তুর বাবার কান্দে ফালাইয়া, বান্দির বাচ্চা রেন্ডির সন্তান,', 'ভারতে মুসলমান খেদাও আন্দোলনের নামে প্রচার করা হচ্ছে মুসলিম বিদ্বেষী পোস্টার-লিফলেট']\n",
            "Total samples after cleaning: 5698\n",
            "First 10 cleaned texts: ['বৌদির দুধ দেকে তো আমার ই চোখ ঠিক ছিলো না  পোলাপান এর চোখ কিভাবে ঠিক থাকবে', 'এই সরকার কে যারা নির্লজ্জের মত সাপোর্ট দিয়েছে বছরের পর বছর তাদের আরো এমন রাস্তায় রাস্তায় কাঁদতে হবে ', 'পিলখানা হত্যাকান্ড বাংলাদেশের প্রতিরক্ষা ব্যবস্থা ধ্বংসের জন্য ভারতের প্রত্যক্ষ সহযোগিতায় এই হত্যাকা ঘটানো হয়েছিল ', 'ভারতের অর্থনীতি নিয়ে আপনাদের ভাবতে হবে না। ভারতের অর্থনীতি নিয়ে ভারত সরকার আছে। আদার বেপারী জাহাজের খোঁজ নিয়ে লাভ নাই।', 'খানকির পুলা মালায়নদের মেরে সাফা করে ফেল', 'শুয়োর এর বাচ্চা দুরগারে চুদি কুত্তা দিয়া কালিরে চুদি পাঠা দিয়া তোর মারে চুদি আমার সোনা দিয়া সিং', 'মাগীর পাছায় লাথি মেরে ওই প্লাটফর্মে রেখে আসা উচিত ', 'আমরা তো দেখলাম ইউটিউব এর মাধ্যমে গাড়ি আওয়ামী লীগ এর লোকজনে পোড়ালো। অথচ মামলা করে দিলো বিএনপির নামে।', 'সানিউর কুত্তার বাচ্চা তুর বোনেরে চুদমু তুর কান্দে ফালাইয়া তুর মাইরে চুদমো তুর বাবার কান্দে ফালাইয়া বান্দির বাচ্চা রেন্ডির সন্তান', 'ভারতে মুসলমান খেদাও আন্দোলনের নামে প্রচার করা হচ্ছে মুসলিম বিদ্বেষী পোস্টারলিফলেট']\n"
          ]
        }
      ],
      "source": [
        "def load_data_and_labels(filename, stop_word_list):\n",
        "    df = pd.read_csv(filename, encoding=\"utf-8\")  # Load CSV properly\n",
        "\n",
        "    x_doc = df[\"text\"].tolist()  # Extract text\n",
        "    label_names = df[\"label\"].tolist()  # Extract labels\n",
        "\n",
        "    categories_dict = {\n",
        "        \"Personal\": 0,\n",
        "        \"Political\": 1,\n",
        "        \"Religious\": 2,\n",
        "        \"Geopolitical\": 3\n",
        "    }  # Define your category mapping\n",
        "\n",
        "    labels = []\n",
        "    for label_name in label_names:\n",
        "        if label_name not in categories_dict:\n",
        "            print(f\"Warning: Label '{label_name}' not found in categories_dict\")\n",
        "            continue  # Skip unknown labels\n",
        "        labels.append(categories_dict[label_name])\n",
        "\n",
        "    print(f\"Total samples before cleaning: {len(x_doc)}\")  # Check if data is loaded\n",
        "    print(\"First 10 raw texts:\", x_doc[:10])  # See raw data before cleaning\n",
        "\n",
        "    trainX = clean_doc(x_doc, stop_word_list)  # Clean text\n",
        "\n",
        "    print(f\"Total samples after cleaning: {len(trainX)}\")  # Check if cleaning is removing everything\n",
        "    print(\"First 10 cleaned texts:\", trainX[:10])  # See processed data\n",
        "\n",
        "    return trainX, labels\n",
        "\n",
        "X_train, y_train = load_data_and_labels('/content/drive/MyDrive/Thesis/bengali_hate_v2.0.csv', stop_word_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "oLWILfX51xdE"
      },
      "outputs": [],
      "source": [
        "def create_tokenizer(lines):\n",
        "    tokenizer=Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "def encode_text(tokenizer, lines, length):\n",
        "    #print(lines)\n",
        "    encoded=tokenizer.texts_to_sequences(lines)\n",
        "    print(encoded[0:10])\n",
        "    padded= pad_sequences(encoded, maxlen=length, padding='post')\n",
        "    return padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5zHs3Juo1xdE"
      },
      "outputs": [],
      "source": [
        "train_tokenizer=create_tokenizer(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKy-xEZ61xdE",
        "outputId": "d2e0b93e-b247-4fed-f91b-ed3f5bf87d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max document length: 300\n",
            "Vocabulary size: 19310\n",
            "\n",
            "[[4369, 352, 3218, 11, 36, 290, 795, 107, 338, 1, 2584, 24, 795, 271, 107, 298], [4, 84, 9, 105, 4370, 44, 2585, 358, 669, 96, 376, 28, 253, 114, 415, 415, 3219, 18], [2175, 3220, 33, 1875, 1209, 2586, 14, 8, 2587, 4371, 4, 6995, 4372, 907], [8, 2588, 281, 259, 2176, 18, 60, 8, 2588, 281, 6, 84, 328, 6996, 6997, 6998, 4373, 281, 299, 239], [78, 519, 4374, 144, 6999, 2, 1342], [1668, 24, 62, 7000, 344, 267, 1102, 4375, 344, 1669, 1102, 31, 307, 344, 36, 796, 1102, 2177], [606, 4376, 429, 144, 240, 4377, 743, 744, 123], [48, 11, 854, 1876, 24, 520, 3221, 145, 151, 24, 7001, 7002, 329, 1103, 2, 1670, 1025, 7003], [7004, 215, 62, 544, 4378, 4379, 544, 4380, 4381, 544, 1492, 7005, 544, 640, 4380, 4381, 2589, 62, 7006, 447], [47, 143, 7007, 1877, 254, 607, 30, 102, 40, 1878, 7008]]\n",
            "(5698, 300)\n"
          ]
        }
      ],
      "source": [
        "trainLength=300\n",
        "vocab_size=len(train_tokenizer.word_index)+1\n",
        "print('Max document length: %d' % trainLength)\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "print()\n",
        "X_train = encode_text(train_tokenizer, X_train, trainLength)\n",
        "print(X_train.shape)\n",
        "#trainX[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "LMHSmNrx1xdE",
        "outputId": "13876f15-8bc0-4a94-930a-993284785091"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unable to handle scheme 'c', expected one of ('', 'file', 'ftp', 'ftps', 'gs', 'hdfs', 'http', 'https', 'viewfs', 'webhdfs'). Extra dependencies required by 'c' may be missing. See <https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst> for details.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-029a157bc21d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/Users/admin-karim/Desktop/BengWord2Vec/posts.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m         \"\"\"\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1954\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                 \u001b[0mrethrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \"\"\"\n\u001b[0;32m-> 1459\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed because loading from S3 doesn't support readline()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_binary_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     filename = (\n\u001b[1;32m    226\u001b[0m         \u001b[0mbinary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_open_binary_stream\u001b[0;34m(uri, mode, transport_params)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mscheme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sniff_scheme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     \u001b[0msubmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/transport.py\u001b[0m in \u001b[0;36mget_transport\u001b[0;34m(scheme)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_REGISTRY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_REGISTRY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unable to handle scheme 'c', expected one of ('', 'file', 'ftp', 'ftps', 'gs', 'hdfs', 'http', 'https', 'viewfs', 'webhdfs'). Extra dependencies required by 'c' may be missing. See <https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst> for details."
          ]
        }
      ],
      "source": [
        "word_vectors = Word2Vec.load('C:/Users/admin-karim/Desktop/BengWord2Vec/posts.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy2kB7l11xdE",
        "outputId": "86c8d756-70cd-4bd2-b9c0-d8475cfbdbbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ],
      "source": [
        "NUM_WORDS=20000\n",
        "EMBEDDING_DIM=300\n",
        "\n",
        "vocabulary_size=len(train_tokenizer.word_index)+1\n",
        "word_index=tokenizer.word_index\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i>=NUM_WORDS:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector=word_vectors[word]\n",
        "        embedding_matrix[i]=embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del(word_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3pzybfP1xdE"
      },
      "outputs": [],
      "source": [
        "embedding_layer=Embedding(vocabulary_size, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCZDCs6P1xdE"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "\t# channel 1\n",
        "\tinput1 = Input(shape=(length,))\n",
        "\tembedding_layer_1 = embedding_layer(input1)\n",
        "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding_layer_1)\n",
        "\tdrop1 = Dropout(0.5)(conv1)\n",
        "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "\tflat1 = Flatten()(pool1)\n",
        "\n",
        "\t# channel 2\n",
        "\tinput2 = Input(shape=(length,))\n",
        "\tembedding_layer_2 = embedding_layer(input2)\n",
        "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding_layer_2)\n",
        "\tdrop2 = Dropout(0.5)(conv2)\n",
        "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "\tflat2 = Flatten()(pool2)\n",
        "\n",
        "\t# channel 3\n",
        "\tinput3 = Input(shape=(length,))\n",
        "\tembedding_layer_3 = embedding_layer(input3)\n",
        "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding_layer_3)\n",
        "\tdrop3 = Dropout(0.5)(conv3)\n",
        "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "\tflat3 = Flatten()(pool3)\n",
        "\n",
        "\t# merge\n",
        "\tCNN_layer = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "\t# LSTM\n",
        "\tx = embedding_layer(input3)\n",
        "\tLSTM_layer = LSTM(128)(x)\n",
        "\n",
        "\tCNN_LSTM_layer = concatenate([LSTM_layer, CNN_layer])\n",
        "\n",
        "\t# interpretation\n",
        "\tdense1 = Dense(10, activation='relu')(CNN_LSTM_layer)\n",
        "\toutputs = Dense(num_of_labels, activation='softmax')(dense1)\n",
        "\tmodel = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
        "\n",
        "\t# compile\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\t# summarize\n",
        "\tprint(model.summary())\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GTke_m71xdE",
        "outputId": "68ebb7db-69d9-4345-87ce-767ffb9f727d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 300)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 300)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 300)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 300, 300)     1025100     input_4[0][0]                    \n",
            "                                                                 input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 297, 32)      38432       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 295, 32)      57632       embedding_2[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 293, 32)      76832       embedding_2[2][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 297, 32)      0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 295, 32)      0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 293, 32)      0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 148, 32)      0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 147, 32)      0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 146, 32)      0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 4736)         0           max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 4704)         0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 4672)         0           max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 128)          219648      embedding_2[3][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 14112)        0           flatten_4[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 14240)        0           lstm_2[0][0]                     \n",
            "                                                                 concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           142410      concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 6)            66          dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,560,120\n",
            "Trainable params: 535,020\n",
            "Non-trainable params: 1,025,100\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model= define_model(EMBEDDING_DIM,vocabulary_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVnJ64Qk1xdF"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "def toCategorical(y):\n",
        "    y = to_categorical(y)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcwmpAdN1xdF"
      },
      "outputs": [],
      "source": [
        "y_train = toCategorical(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzgaRAqb1xdF",
        "outputId": "01c67e46-3452-498d-9a26-b2b8f5ebd013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 701 samples, validate on 176 samples\n",
            "Epoch 1/20\n",
            "701/701 [==============================] - 12s 17ms/step - loss: 1.6760 - acc: 0.2539 - val_loss: 3.3705 - val_acc: 0.0398\n",
            "Epoch 2/20\n",
            "701/701 [==============================] - 9s 14ms/step - loss: 1.3261 - acc: 0.4579 - val_loss: 5.2184 - val_acc: 0.0000e+00\n",
            "Epoch 3/20\n",
            "701/701 [==============================] - 9s 13ms/step - loss: 1.1300 - acc: 0.5449 - val_loss: 6.8787 - val_acc: 0.0000e+00\n",
            "Epoch 4/20\n",
            "701/701 [==============================] - 12s 16ms/step - loss: 1.0058 - acc: 0.5877 - val_loss: 8.1012 - val_acc: 0.0000e+00\n",
            "Epoch 5/20\n",
            "701/701 [==============================] - 11s 16ms/step - loss: 0.8959 - acc: 0.6391 - val_loss: 9.2797 - val_acc: 0.0000e+00\n",
            "Epoch 6/20\n",
            "701/701 [==============================] - 22s 32ms/step - loss: 0.8244 - acc: 0.6534 - val_loss: 9.5983 - val_acc: 0.0000e+00\n",
            "Epoch 7/20\n",
            "701/701 [==============================] - 24s 34ms/step - loss: 0.7083 - acc: 0.7361 - val_loss: 9.9701 - val_acc: 0.0057\n",
            "Epoch 8/20\n",
            "701/701 [==============================] - 28s 40ms/step - loss: 0.6185 - acc: 0.7575 - val_loss: 10.3135 - val_acc: 0.0114\n",
            "Epoch 9/20\n",
            "701/701 [==============================] - 28s 40ms/step - loss: 0.5373 - acc: 0.8160 - val_loss: 10.8100 - val_acc: 0.0057\n",
            "Epoch 10/20\n",
            "701/701 [==============================] - 20s 29ms/step - loss: 0.4488 - acc: 0.8374 - val_loss: 11.3847 - val_acc: 0.0000e+00\n",
            "Epoch 11/20\n",
            "701/701 [==============================] - 23s 33ms/step - loss: 0.3866 - acc: 0.8688 - val_loss: 11.7607 - val_acc: 0.0000e+00\n",
            "Epoch 12/20\n",
            "701/701 [==============================] - 26s 38ms/step - loss: 0.3355 - acc: 0.8902 - val_loss: 12.0179 - val_acc: 0.0000e+00\n",
            "Epoch 13/20\n",
            "701/701 [==============================] - 23s 32ms/step - loss: 0.2882 - acc: 0.9301 - val_loss: 12.1809 - val_acc: 0.0057\n",
            "Epoch 14/20\n",
            "701/701 [==============================] - 21s 29ms/step - loss: 0.2529 - acc: 0.9358 - val_loss: 12.3040 - val_acc: 0.0000e+00\n",
            "Epoch 15/20\n",
            "701/701 [==============================] - 19s 27ms/step - loss: 0.2402 - acc: 0.9201 - val_loss: 12.4246 - val_acc: 0.0057\n",
            "Epoch 16/20\n",
            "701/701 [==============================] - 18s 25ms/step - loss: 0.2199 - acc: 0.9458 - val_loss: 12.3804 - val_acc: 0.0114\n",
            "Epoch 17/20\n",
            "701/701 [==============================] - 21s 30ms/step - loss: 0.1884 - acc: 0.9458 - val_loss: 12.5150 - val_acc: 0.0000e+00\n",
            "Epoch 18/20\n",
            "701/701 [==============================] - 20s 29ms/step - loss: 0.1621 - acc: 0.9643 - val_loss: 12.4372 - val_acc: 0.0227\n",
            "Epoch 19/20\n",
            "701/701 [==============================] - 20s 28ms/step - loss: 0.1485 - acc: 0.9686 - val_loss: 12.6193 - val_acc: 0.0227\n",
            "Epoch 20/20\n",
            "701/701 [==============================] - 22s 32ms/step - loss: 0.1394 - acc: 0.9672 - val_loss: 12.6117 - val_acc: 0.0057\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1480ceb9630>"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit([X_train,X_train,X_train], y_train, epochs=20, batch_size=128, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7rdR8P81xdF"
      },
      "outputs": [],
      "source": [
        "# save the model\n",
        "model.save('MConv_LSTM_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDGqfEt91xdF"
      },
      "outputs": [],
      "source": [
        "X_test, y_test = load_data_and_labels('Hate_Speech_Test.csv',stop_word_list)\n",
        "y_test = toCategorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66uC2bN81xdF",
        "outputId": "6c7744ac-e747-4615-bb33-3b47e5f82e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max document length: 300\n",
            "Vocabulary size: 1616\n",
            "\n",
            "[[222, 409, 410, 411, 412, 22], [145, 47, 81, 48, 146, 413, 414, 82, 109], [415, 223, 42, 224, 5, 416], [417, 418, 419, 225, 23], [110, 10, 420, 226, 227, 421, 228, 422, 423, 424, 229, 230, 147, 231, 1, 36, 425, 37, 426], [148, 427, 38, 428, 232, 83, 111, 38, 429, 1], [26, 233, 430, 149], [431, 432, 14, 433, 434], [435, 23, 234, 235, 236, 436], [26, 17, 47]]\n",
            "(323, 300)\n",
            "Test accuracy: 30.650155\n"
          ]
        }
      ],
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "\n",
        "tokenizer_test = create_tokenizer(X_test)\n",
        "\n",
        "trainLength=300\n",
        "vocab_size=len(tokenizer_test.word_index)+1\n",
        "print('Max document length: %d' % trainLength)\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "print()\n",
        "X_test = encode_text(tokenizer_test, X_test, trainLength)\n",
        "print(X_test.shape)\n",
        "#trainX[2]\n",
        "\n",
        "# load the model\n",
        "model = load_model('MConv_LSTM_model.h5')\n",
        "\n",
        "# evaluate model on test dataset dataset\n",
        "loss, acc = model.evaluate([X_test,X_test,X_test], y_test, verbose=0)\n",
        "print('Test accuracy: %f' % (acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKYATFk11xdF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}